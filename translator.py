# -*- coding: utf-8 -*-
"""Translator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/134LSUdEGu6ZTd8SkwBgHwLC1r6uXL7YS
"""

pip install googletrans==4.0.0-rc1

pip install PyMuPDF

pip install reportlab

pip install python-docx

from IPython import get_ipython
from IPython.display import display
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

!pip install spacy
!python -m spacy download en_core_web_sm

import spacy

nlp = spacy.load("en_core_web_sm")

def keywords_spaCy(text):
  doc = nlp(text)
  target_pos = {"NOUN", "VERB", "ADJ", "ADV"}
  filtered = [token.text for token in doc if token.pos_ in target_pos]
  #print("Original:", text)
  return filtered

keywords_spaCy("This is an example of removing stopwords from a sentence.")

def remove_duplicates_case_insensitive(strings):
    seen = set()
    result = []
    for s in strings:
        key = s.lower()  # case insensitive
        if key not in seen:
            seen.add(key)
            result.append(s)
    return result

## def remove_stopwords(sentence):
##   words = word_tokenize(sentence)
##   filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]
##   #print("Original:", sentence)
##   filtered = " ".join(filtered_words)
##   no_punct = filtered.translate(str.maketrans('', '', string.punctuation))
##   #print("Filtered: ", no_punct)
##   return no_punct

## remove_stopwords("This is an example of removing stopwords from a sentence.")

## def split_up(filtered):
##   list = filtered.split(' ')
##   while '' in list:
##     list.remove('')
##   print(list)
##   return list

## split_up(remove_stopwords("This is an example of removing stopwords from a sentence."))

"""Code to translate the individual words below:"""

from googletrans import Translator

translator = Translator()

def translate(lang, list):
  translated_words = [translator.translate(word, dest=lang).text for word in list]
  translation_list = []
  for original, translated in zip(list, translated_words):
    translation_list.append((f"{original:<15} â†’        {translated}"))
  return translation_list

## translate("ta", split_up(remove_stopwords("Place the People rectangles in the circles on the Data Privacy Spheres sheet starting with the people that you consider closest to you in circle 1 and the people least close to you in circle 7. ")))

translate("ta", keywords_spaCy("Place the People rectangles in the circles on the Data Privacy Spheres sheet starting with the people that you consider closest to you in circle 1 and the people least close to you in circle 7. "))

"""Code for extracting text"""

import fitz

from google.colab import files

# Upload the PDF
uploaded = files.upload()

# The uploaded file will be stored in the '/content/' directory.
pdf_path = list(uploaded.keys())[0]  # Get the file name
print(f"PDF file uploaded: {pdf_path}")

def extract(pdf_path):
  text = ""
  with fitz.open(pdf_path) as doc:
        for page_num, page in enumerate(doc):
            text += page.get_text()
  return text

for filename in uploaded.keys():
    print(f"Uploaded file name: {filename}")

# translate("ta", keywords_spaCy(extract(filename)))

"""Export as PDF"""

from reportlab.lib.pagesizes import LETTER
from reportlab.pdfgen import canvas

from googletrans import LANGUAGES

from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from google.colab import files

# Download a NotoSans font file (replace with a suitable URL if needed)
!wget -O NotoSans-Regular.ttf https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSans/NotoSans-Regular.ttf

# Register the NotoSans font
pdfmetrics.registerFont(TTFont('NotoSans', 'NotoSans-Regular.ttf'))

uploaded = files.upload()
pdfmetrics.registerFont(TTFont('ArialUnicode', 'ArialUnicodeMS copy.ttf'))

def export_text_to_pdf(output_filename, text_lines):
    # Create a new PDF canvas
    c = canvas.Canvas(output_filename, pagesize=LETTER)
    width, height = LETTER

    # Set starting position
    x = 50
    y = height - 50

    c.setFont('ArialUnicode', 12)

    # Write each line to the PDF
    for line in text_lines:
        c.drawString(x, y, line)
        y -= 15

        # new page
        if y < 50:
            c.showPage()
            c.setFont('ArialUnicode', 12)  # Reset font on new page
            y = height - 50

    # Save the PDF
    c.save()
    files.download(output_filename)

export_text_to_pdf("output.pdf", translate("ta", keywords_spaCy(extract("examplePDF.pdf"))))

def create_pdf(language, file):
  language_code = next((code for code, name in LANGUAGES.items() if name.lower() == language.lower()), None)
  if (language_code is None):
    print("Invalid language name")
    return
  translated_text = remove_duplicates_case_insensitive(translate(language_code, keywords_spaCy(extract(file))))
  filename = file.removesuffix(".pdf") + language + ".pdf"
  export_text_to_pdf(filename, translated_text)

create_pdf("arabic", "examplePDF.pdf")

"""GUI"""

from google.colab import files
import ipywidgets as widgets
from IPython.display import display

# Upload file
uploaded = files.upload()

# Language input
lang_input = widgets.Text(
    value='',
    placeholder='e.g. French',
    description='Language:',
    disabled=False
)

# Define what happens when Enter is pressed
def on_enter(change):
    if change['name'] == 'value' and change['type'] == 'change':
        print(f"Language entered: {change['new']}")

# Attach the event handler
lang_input.observe(on_enter)

# Display the widget
display(lang_input)
